{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3dd2796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBatch Normalization\\nThis notebook contains an implementation of a deep fully-connected neural network with batch normalization,\\napplied to the problem of multi-class classification of handwritten digits.\\nThe training and test sets are ready to use as part of the MNIST dataset containing greyscale images of digits.\\nLink to the dataset: http://yann.lecun.com/exdb/mnist/\\n\\nThe original reason for making this notebook was to practice batch normalization by implementing it from scratch.\\nThis happened while I was following the Deep Learning specialization on Coursera,\\nspecifically the second course called \"Improving Deep Neural Networks\",\\nin which Batch normalization is described but not implemented as part of an assignment.\\n\\nImplemented from scratch with extensive use of the Numpy library, the network contains:\\n- Configurable number of layers and units for each layer,\\n- ReLU activations for all layers except the last, while the last layer uses the Softmax activation,\\n- Categorical cross-entropy loss function,\\n- Adam optimizer as a gradient descent based optimization method,\\n- Mini-batches,\\n- Batch-normalization,\\n- Learning rate decay.\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Batch Normalization\n",
    "This notebook contains an implementation of a deep fully-connected neural network with batch normalization,\n",
    "applied to the problem of multi-class classification of handwritten digits.\n",
    "The training and test sets are ready to use as part of the MNIST dataset containing greyscale images of digits.\n",
    "Link to the dataset: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "The original reason for making this notebook was to practice batch normalization by implementing it from scratch.\n",
    "This happened while I was following the Deep Learning specialization on Coursera,\n",
    "specifically the second course called \"Improving Deep Neural Networks\",\n",
    "in which Batch normalization is described but not implemented as part of an assignment.\n",
    "\n",
    "Implemented from scratch with extensive use of the Numpy library, the network contains:\n",
    "- Configurable number of layers and units for each layer,\n",
    "- ReLU activations for all layers except the last, while the last layer uses the Softmax activation,\n",
    "- Categorical cross-entropy loss function,\n",
    "- Adam optimizer as a gradient descent based optimization method,\n",
    "- Mini-batches,\n",
    "- Batch-normalization,\n",
    "- Learning rate decay.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1611d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229d1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09129582",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/train-images.idx3-ubyte\", 'rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    n_rows, n_cols = struct.unpack(\">II\", f.read(8))\n",
    "    x_train_orig = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).reshape((size, n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fcf029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_orig[size-1, :, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3784982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train examples: 60000\n",
      "rows: 28\n",
      "cols: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"number of train examples: \" + str(size))\n",
    "print(\"rows: \" + str(n_rows))\n",
    "print(\"cols: \" + str(n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7eb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a9b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/train-labels.idx1-ubyte\", 'rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    y_train_orig = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be5a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train labels: 60000\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(\"number of train labels: \" + str(size))\n",
    "print(y_train_orig[0, size-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535d121a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(y_train_orig.reshape((-1,)), 10, facecolor='lightblue', edgecolor='black')\n",
    "plt.title('Train set labels')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2129d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEST EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "030fee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/t10k-images.idx3-ubyte\", 'rb') as f:\n",
    "    magic, size_test = struct.unpack(\">II\", f.read(8))\n",
    "    n_rows, n_cols = struct.unpack(\">II\", f.read(8))\n",
    "    x_test_orig = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).reshape((size_test, n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59159a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEST LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2761a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/t10k-labels.idx1-ubyte\", 'rb') as f:\n",
    "    magic, size_test = struct.unpack(\">II\", f.read(8))\n",
    "    y_test_orig = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772ebc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_orig[5000, :, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dadc4b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(y_test_orig.reshape((-1,)), 10, facecolor='lightblue', edgecolor='black')\n",
    "plt.title('Test set labels')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfe808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "# The \"-1\" makes reshape flatten the remaining dimensions\n",
    "x_train_flat = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n",
    "x_test_flat = x_test_orig.reshape(x_test_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51587aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  5  5]\n",
      " [ 2  2  2]\n",
      " [-1 -1 -1]\n",
      " [ 3  3  3]]\n",
      "(array([[0.99330715, 0.99330715, 0.99330715],\n",
      "       [0.88079708, 0.88079708, 0.88079708],\n",
      "       [0.26894142, 0.26894142, 0.26894142],\n",
      "       [0.95257413, 0.95257413, 0.95257413]]), array([[ 5,  5,  5],\n",
      "       [ 2,  2,  2],\n",
      "       [-1, -1, -1],\n",
      "       [ 3,  3,  3]]))\n",
      "(array([[5, 5, 5],\n",
      "       [2, 2, 2],\n",
      "       [0, 0, 0],\n",
      "       [3, 3, 3]]), array([[ 5,  5,  5],\n",
      "       [ 2,  2,  2],\n",
      "       [-1, -1, -1],\n",
      "       [ 3,  3,  3]]))\n",
      "(array([[0.84203357, 0.84203357, 0.84203357],\n",
      "       [0.04192238, 0.04192238, 0.04192238],\n",
      "       [0.00208719, 0.00208719, 0.00208719],\n",
      "       [0.11395685, 0.11395685, 0.11395685]]), array([[ 5,  5,  5],\n",
      "       [ 2,  2,  2],\n",
      "       [-1, -1, -1],\n",
      "       [ 3,  3,  3]]))\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z)), Z\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(Z, 0), Z\n",
    "\n",
    "def softmax(Z):\n",
    "    Z_shifted = Z - np.max(Z)\n",
    "    T = np.exp(Z_shifted)\n",
    "    return T / np.sum(T, axis=0), Z\n",
    "\n",
    "\n",
    "mat = np.array([[5, 2, -1, 3], [5, 2, -1, 3], [5, 2, -1, 3]]).T\n",
    "print(mat)\n",
    "\n",
    "print(sigmoid(mat))\n",
    "print(relu(mat))\n",
    "print(softmax(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6034c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4 6 6]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(labels, depth):\n",
    "    one_hot = np.zeros((depth, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "why_samples = np.array([2, 4, 6, 6]).reshape((1, -1))\n",
    "print(why_samples)\n",
    "print(one_hot_encoding(why_samples, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab1e1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(AL, Y_one_hot):\n",
    "    \"\"\"\n",
    "    The categorial cross-entropy cost function that computes the losses from aL and y.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (num_classes, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing a digit), shape (num_classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y_one_hot.shape[1]\n",
    "    cost = -1 / m * np.sum(Y_one_hot * np.log(AL + 1e-15))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1de12ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 256):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((-1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_mini_batches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_mini_batches):\n",
    "        mini_batch_X = shuffled_X[:, k * inc:(k+1) * inc]\n",
    "        mini_batch_Y = shuffled_Y[:, k * inc:(k+1) * inc]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, inc * num_complete_mini_batches:]\n",
    "        mini_batch_Y = shuffled_Y[:, inc * num_complete_mini_batches:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9aa1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "    \n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer\n",
    "    decay_rate -- Decay rate. Scalar\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar \n",
    "    \"\"\"\n",
    "    learning_rate = 1 / (1 + decay_rate * epoch_num) * learning_rate0\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "516207d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=10):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "    \n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer.\n",
    "    decay_rate -- Decay rate. Scalar.\n",
    "    time_interval -- Number of epochs where you update the learning rate.\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar \n",
    "    \"\"\"\n",
    "    learning_rate = 1 / (1 + decay_rate * np.floor(epoch_num / time_interval)) * learning_rate0\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d24f5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"beta1\", \"gamma1\", ..., \"WL\", \"betaL\", \"gammaL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    betal -- bias vector of shape (layer_dims[l], 1)\n",
    "                    gammal -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # For ReLU layers initialize weights by multiplying gaussian with np.sqrt(2.0 / layer_dims[l-1])\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "        parameters['beta' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        parameters['gamma' + str(l)] = np.ones((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8554416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(Z, beta, gamma, running_mean = None, running_var = None, epsilon = 1e-15):\n",
    "    \"\"\"\n",
    "    During training, mini-batch mean and variance is calculated from the input.\n",
    "    During testing, running mean and variance is provided instead.\n",
    "    \"\"\"\n",
    "    m = Z.shape[1]\n",
    "    \n",
    "    if running_mean is None or running_var is None:\n",
    "        mean = 1. / m * np.sum(Z, axis=1, keepdims=True)\n",
    "        Zmean = Z - mean\n",
    "        sq = Zmean**2\n",
    "        var = 1. / m * np.sum(sq, axis=1, keepdims=True)\n",
    "    else:\n",
    "        mean = running_mean\n",
    "        Zmean = Z - mean\n",
    "        sq = Zmean**2\n",
    "        var = running_var\n",
    "    \n",
    "    sqrtvar = np.sqrt(var + epsilon)\n",
    "    ivar = 1./sqrtvar\n",
    "    Z_normalized = Zmean * ivar\n",
    "    Z_gamma = gamma * Z_normalized\n",
    "    Z_tilde = Z_gamma + beta\n",
    "    \n",
    "    cache = (Z, Z_normalized, gamma, Zmean, ivar, sqrtvar, epsilon, mean, var)\n",
    "    \n",
    "    return Z_tilde, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a6d7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", and \"W\"; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A\n",
    "    cache = (A, W)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6313616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, beta, gamma, activation, layer_mean, layer_var):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W)\n",
    "    \n",
    "    # normalize mini-batch\n",
    "    Z_tilde, batch_norm_cache = batch_norm(Z, beta, gamma, layer_mean, layer_var)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z_tilde)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z_tilde)\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z_tilde)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache, batch_norm_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c07108d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, L, means_per_layer = None, variances_per_layer = None):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SOFTMAX] computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    # means and variances per layer (relevant only during training)\n",
    "    means = []\n",
    "    variances = []\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        layer_mean = None if means_per_layer is None else means_per_layer[l-1]\n",
    "        layer_var = None if variances_per_layer is None else variances_per_layer[l-1] \n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['beta' + str(l)],\n",
    "                                             parameters['gamma' + str(l)], 'relu', layer_mean, layer_var)\n",
    "        _, _, batch_norm_cache = cache\n",
    "        _, _, _, _, _, _, _, mean, var = batch_norm_cache\n",
    "        means.append(mean)\n",
    "        variances.append(var)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n",
    "    layer_mean = None if means_per_layer is None else means_per_layer[L-1]\n",
    "    layer_var = None if variances_per_layer is None else variances_per_layer[L-1]\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['beta' + str(L)],\n",
    "                                          parameters['gamma' + str(L)], 'softmax', layer_mean, layer_var)\n",
    "    _, _, batch_norm_cache = cache\n",
    "    _, _, _, _, _, _, _, mean, var = batch_norm_cache\n",
    "    means.append(mean)\n",
    "    variances.append(var)\n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches, means, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ae7a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y_one_hot, parameters, lambd):\n",
    "    m = Y_one_hot.shape[1]\n",
    "    \n",
    "    cross_entropy = categorical_cross_entropy(AL, Y_one_hot)\n",
    "    \n",
    "    # Add L2 regularization cost based on parameters W\n",
    "    num_layers = len(parameters) // 3\n",
    "    all_W = np.concatenate(tuple([parameters[\"W\" + str(l)] for l in range(1, num_layers+1)]), axis=None)\n",
    "    L2_regularization_cost = lambd / (2 * m) * np.sum(np.square(all_W))\n",
    "    \n",
    "    return cross_entropy + L2_regularization_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2397f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_softmax_layer_backward(Y_one_hot, AL, cache, lambd):\n",
    "    linear_cache, _, batch_norm_cache = cache\n",
    "    \n",
    "    dZL_tilde = AL - Y_one_hot\n",
    "    dZL, dbeta, dgamma = batch_norm_backward(dZL_tilde, batch_norm_cache)\n",
    "    dA_prev, dW = linear_backward(dZL, linear_cache, lambd)\n",
    "    \n",
    "    return dA_prev, dW, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f05b2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation, lambd):\n",
    "    linear_cache, activation_cache, batch_norm_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ_tilde = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ_tilde = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dZ, dbeta, dgamma = batch_norm_backward(dZ_tilde, batch_norm_cache)\n",
    "    dA_prev, dW = linear_backward(dZ, linear_cache, lambd)\n",
    "    \n",
    "    return dA_prev, dW, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f4b7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    s, _ = sigmoid(Z)\n",
    "    return dA * s * (1 - s)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    return dA * np.greater(Z, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e99792cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_backward(dZ_tilde, batch_norm_cache):\n",
    "    Z, Z_normalized, gamma, Zmean, ivar, sqrtvar, epsilon, mean, var = batch_norm_cache\n",
    "    \n",
    "    m = dZ_tilde.shape[1]\n",
    "    \n",
    "    dbeta = np.sum(dZ_tilde, axis=1, keepdims=True)\n",
    "    dgamma = np.sum(dZ_tilde * Z_normalized, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ_normalized = dZ_tilde * gamma\n",
    "    divar = np.sum(dZ_normalized * Zmean, axis=1, keepdims=True)\n",
    "    dxmu1 = dZ_normalized * ivar\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "    dvar = 0.5 * 1. /np.sqrt(var + epsilon) * dsqrtvar\n",
    "    dsq = 1. / m * np.ones(dZ_tilde.shape) * dvar\n",
    "    dxmu2 = 2 * Zmean * dsq\n",
    "    dZ1 = dxmu1 + dxmu2\n",
    "    dmu = -1 * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    dZ2 = 1. / m * np.ones(dZ_tilde.shape) * dmu\n",
    "    dZ = dZ1 + dZ2\n",
    "\n",
    "    #t = 1. / np.sqrt(var + eps)\n",
    "    #dZ = (gamma * t / m) * (m * dZ_tilde - dbeta - t**2 * (Z - mean) * np.sum(dZ_tilde * (Z - mean), axis=1, keepdims=True))\n",
    "    \n",
    "    return dZ, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d35d2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache, lambd):\n",
    "    A_prev, W = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1.0/m * dZ @ A_prev.T + lambd / m * W\n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    return dA_prev, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f4ff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, lambd):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, dbeta_temp, dgamma_temp = final_softmax_layer_backward(Y, AL, current_cache, lambd)\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"dbeta\" + str(L)] = dbeta_temp\n",
    "    grads[\"dgamma\" + str(L)] = dgamma_temp\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, dbeta_temp, dgamma_temp = linear_activation_backward(dA_prev_temp, current_cache, 'relu', lambd)\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"dbeta\" + str(l + 1)] = dbeta_temp\n",
    "        grads[\"dgamma\" + str(l + 1)] = dgamma_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "472edd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 3 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "        W_shape = parameters[\"W\" + str(l)].shape\n",
    "        beta_shape = parameters[\"beta\" + str(l)].shape\n",
    "        gamma_shape = parameters[\"gamma\" + str(l)].shape\n",
    "        v[\"dW\" + str(l)] = np.zeros(W_shape)\n",
    "        v[\"dbeta\" + str(l)] = np.zeros(beta_shape)\n",
    "        v[\"dgamma\" + str(l)] = np.zeros(gamma_shape)\n",
    "        s[\"dW\" + str(l)] = np.zeros(W_shape)\n",
    "        s[\"dbeta\" + str(l)] = np.zeros(beta_shape)\n",
    "        s[\"dgamma\" + str(l)] = np.zeros(gamma_shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38121de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate,\n",
    "                                beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- Adam variable, counts the number of taken steps\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 3                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads[\"dW\" + str(l)]\n",
    "        v[\"dbeta\" + str(l)] = beta1 * v[\"dbeta\" + str(l)] + (1 - beta1) * grads[\"dbeta\" + str(l)]\n",
    "        v[\"dgamma\" + str(l)] = beta1 * v[\"dgamma\" + str(l)] + (1 - beta1) * grads[\"dgamma\" + str(l)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - beta1**t)\n",
    "        v_corrected[\"dbeta\" + str(l)] = v[\"dbeta\" + str(l)] / (1 - beta1**t)\n",
    "        v_corrected[\"dgamma\" + str(l)] = v[\"dgamma\" + str(l)] / (1 - beta1**t)\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1 - beta2) * grads[\"dW\" + str(l)]**2\n",
    "        s[\"dbeta\" + str(l)] = beta2 * s[\"dbeta\" + str(l)] + (1 - beta2) * grads[\"dbeta\" + str(l)]**2\n",
    "        s[\"dgamma\" + str(l)] = beta2 * s[\"dgamma\" + str(l)] + (1 - beta2) * grads[\"dgamma\" + str(l)]**2\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - beta2**t)\n",
    "        s_corrected[\"dbeta\" + str(l)] = s[\"dbeta\" + str(l)] / (1 - beta2**t)\n",
    "        s_corrected[\"dgamma\" + str(l)] = s[\"dgamma\" + str(l)] / (1 - beta2**t)\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * (v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon))\n",
    "        parameters[\"beta\" + str(l)] -= learning_rate * (v_corrected[\"dbeta\" + str(l)] / (np.sqrt(s_corrected[\"dbeta\" + str(l)]) + epsilon))\n",
    "        parameters[\"gamma\" + str(l)] -= learning_rate * (v_corrected[\"dgamma\" + str(l)] / (np.sqrt(s_corrected[\"dgamma\" + str(l)]) + epsilon))\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ac824c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 50,\n",
    "                  lambd = 0.7, momentum = 0.9, print_cost=True, decay=None, decay_rate=1):\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0  # initializing the counter required for Adam update\n",
    "    \n",
    "    Y_one_hot = one_hot_encoding(Y, num_classes)\n",
    "    m = Y.shape[1]\n",
    "    L = len(layer_dims) - 1\n",
    "    \n",
    "    learning_rate0 = learning_rate   # the original learning rate\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_iterations):\n",
    "        mini_batches = random_mini_batches(X, Y_one_hot)\n",
    "        cost_total = 0\n",
    "        running_means_per_layer = [np.zeros((layer_dims[l+1], 1)) for l in range(L)]\n",
    "        running_vars_per_layer = [np.zeros((layer_dims[l+1], 1)) for l in range(L)]\n",
    "        \n",
    "        for b, mini_batch in enumerate(mini_batches):\n",
    "\n",
    "            # Select a minibatch\n",
    "            (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "\n",
    "            # Forward propagation\n",
    "            AL, caches, means_per_layer, variances_per_layer = L_model_forward(mini_batch_X, parameters, L)\n",
    "            \n",
    "            # Update running averages across mini-batches (per layer)\n",
    "            for l in range(L):\n",
    "                running_means_per_layer[l] = momentum * running_means_per_layer[l] + (1 - momentum) * means_per_layer[l]\n",
    "                running_vars_per_layer[l] = momentum * running_vars_per_layer[l] + (1 - momentum) * variances_per_layer[l]\n",
    "                \n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(AL, mini_batch_Y, parameters, lambd)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = L_model_backward(AL, mini_batch_Y, caches, lambd)\n",
    "\n",
    "            # Update parameters\n",
    "            t += 1  # Adam counter\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate)\n",
    "            \n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "        \n",
    "        # Store the cost every n number of epochs\n",
    "        if i == 0 or i % 5 == 0 or i == num_iterations - 1:\n",
    "            costs.append(cost_avg)\n",
    "            if print_cost:\n",
    "                print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "    \n",
    "    return parameters, costs, running_means_per_layer, running_vars_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d4d032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, L, means_per_layer, vars_per_layer):\n",
    "    AL, _, _, _ = L_model_forward(X, parameters, L, means_per_layer, vars_per_layer)\n",
    "    return np.argmax(AL, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce8d73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, parameters, L, means_per_layer, vars_per_layer):\n",
    "    return np.sum(predict(X, parameters, L, means_per_layer, vars_per_layer) == Y) / X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fd291ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK BN FORWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04d006a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ 17.38546145 -28.21498249  55.02474629]\n",
      "  stds:  [32.98591027 31.65106098 31.74463317]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [7.10542736e-17 4.88498131e-17 9.76996262e-17]\n",
      "  std:  [1. 1. 1.]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:  [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Credits to Standford course cs231n for this block of code.\n",
    "# https://cs231n.github.io/assignments2022/assignment2/\n",
    "\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "M, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(D1, M)\n",
    "W1 = np.random.randn(D2, D1)\n",
    "W2 = np.random.randn(D3, D2)\n",
    "a = W2.dot(np.maximum(0, W1.dot(X)))\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=1))\n",
    "print('  stds: ', a.std(axis=1))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batch_norm(a, np.zeros((D3, 1)), np.ones((D3, 1)))\n",
    "a_norm_mean = a_norm.mean(axis=1)\n",
    "a_norm_std = a_norm.std(axis=1)\n",
    "print('  mean: ', a_norm_mean)\n",
    "print('  std: ', a_norm_std)\n",
    "\n",
    "assert np.isclose(a_norm_mean, np.zeros(a_norm_mean.shape)).all()\n",
    "assert np.isclose(a_norm_std, np.ones(a_norm_mean.shape)).all()\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "beta = np.asarray([[11.0], [12.0], [13.0]])\n",
    "gamma = np.asarray([[1.0], [2.0], [3.0]])\n",
    "a_norm, _ = batch_norm(a, beta, gamma)\n",
    "a_norm_mean = a_norm.mean(axis=1)\n",
    "a_norm_std = a_norm.std(axis=1)\n",
    "print('After batch normalization (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm_mean)\n",
    "print('  stds: ', a_norm_std)\n",
    "\n",
    "assert np.isclose(a_norm_mean, beta.flatten()).all()\n",
    "assert np.isclose(a_norm_std, gamma.flatten()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed57fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.02951383  0.10758225 -0.13066542]\n",
      "  stds:  [0.91736387 1.04711055 0.95737263]\n"
     ]
    }
   ],
   "source": [
    "# Credits to Standford course cs231n for this block of code.\n",
    "# https://cs231n.github.io/assignments2022/assignment2/\n",
    "\n",
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "M, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D2, D1)\n",
    "W2 = np.random.randn(D3, D2)\n",
    "\n",
    "gamma = np.ones((D3, 1))\n",
    "beta = np.zeros((D3, 1))\n",
    "for t in range(50):\n",
    "    X = np.random.randn(D1, M)\n",
    "    a = W2.dot(np.maximum(0, W1.dot(X)))\n",
    "    a_norm, cache = batch_norm(a, beta, gamma)\n",
    "\n",
    "_, _, _, _, _, _, _, mean, var = cache\n",
    "\n",
    "X = np.random.randn(D1, M)\n",
    "a = W2.dot(np.maximum(0, W1.dot(X)))\n",
    "a_norm, _ = batch_norm(a, beta, gamma, mean, var)\n",
    "a_norm_mean = a_norm.mean(axis=1)\n",
    "a_norm_std = a_norm.std(axis=1)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm_mean)\n",
    "print('  stds: ', a_norm_std)\n",
    "\n",
    "assert np.isclose(a_norm_mean, np.zeros(a_norm_mean.shape), atol=.255).all()\n",
    "assert np.isclose(a_norm_std, np.ones(a_norm_mean.shape), atol=.15).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "205eee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK BN BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0293e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of grad is  (5, 4)\n",
      "Shape of grad is  (5, 1)\n",
      "Shape of grad is  (5, 1)\n",
      "Shape of dx is  (5, 4)\n",
      "dx error:  4.5740574144533545e-10\n",
      "dgamma error:  1.176717928039398e-09\n",
      "dbeta error:  7.871800462910199e-12\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.2156157823905741e-11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Credits to Standford course cs231n for this block of code.\n",
    "# https://cs231n.github.io/assignments2022/assignment2/\n",
    "\n",
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    it.iternext()\n",
    "  print(\"Shape of grad is \", grad.shape)\n",
    "  return grad\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "D, M = 5, 4\n",
    "x = 5 * np.random.randn(D, M) + 12\n",
    "gamma = np.random.randn(D, 1)\n",
    "beta = np.random.randn(D, 1)\n",
    "dout = np.random.randn(D, M)\n",
    "\n",
    "fx = lambda x: batch_norm(x, beta, gamma)[0]\n",
    "fg = lambda a: batch_norm(x, beta, gamma)[0]\n",
    "fb = lambda b: batch_norm(x, beta, gamma)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batch_norm(x, beta, gamma)\n",
    "dx, dbeta, dgamma = batch_norm_backward(dout, cache)\n",
    "print(\"Shape of dx is \", dx.shape)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))\n",
    "\n",
    "grad = np.concatenate((dx.flatten(), dgamma.flatten(), dbeta.flatten()))\n",
    "gradapprox = np.concatenate((dx_num.flatten(), da_num.flatten(), db_num.flatten()))\n",
    "\n",
    "numerator = np.linalg.norm(grad - gradapprox)\n",
    "denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "difference = numerator / denominator\n",
    "\n",
    "if difference > 2e-7:\n",
    "    print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "else:\n",
    "    print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dda444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL WITH BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "784b09f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.005037\n",
      "Cost after epoch 5: 0.001887\n",
      "Cost after epoch 10: 0.001383\n",
      "Cost after epoch 15: 0.001140\n",
      "Cost after epoch 20: 0.001024\n",
      "Cost after epoch 25: 0.000894\n",
      "Cost after epoch 30: 0.001452\n",
      "Cost after epoch 35: 0.001201\n",
      "Cost after epoch 40: 0.001036\n",
      "Cost after epoch 45: 0.000967\n",
      "Cost after epoch 50: 0.000948\n",
      "Cost after epoch 55: 0.000821\n",
      "Cost after epoch 60: 0.000765\n",
      "Cost after epoch 65: 0.000669\n",
      "Cost after epoch 70: 0.000601\n",
      "Cost after epoch 74: 0.000568\n",
      "Train set accuracy: 98.04333333333334%\n",
      "Test set accuracy: 96.78%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = x_train_flat[:,:]/255.\n",
    "y_train = y_train_orig[:,:]\n",
    "x_test = x_test_flat[:,:]/255.\n",
    "y_test = y_test_orig[:,:]\n",
    "\n",
    "#print(\"mean of x_train on a few examples: \" + str(x_train.mean(axis=1, keepdims=True)))\n",
    "#print(\"var of x_train on a few examples: \" + str(x_train.var(axis=1, keepdims=True)))\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_classes = 10\n",
    "layer_dims = [n_rows*n_cols, 110, num_classes]\n",
    "L = len(layer_dims) - 1\n",
    "\n",
    "parameters, costs, means_per_layer, vars_per_layer = L_layer_model(x_train, y_train, layer_dims, learning_rate,\n",
    "                                                                   num_iterations = 75, lambd = 0.7,\n",
    "                                                                   momentum = 0.9, print_cost=True, \n",
    "                                                                   decay=exponential_update_lr, decay_rate=0.6)\n",
    "\n",
    "\n",
    "#print(\"moving mean on a few examples: \" + str(means_per_layer))\n",
    "#print(\"moving var on a few examples: \" + str(vars_per_layer))\n",
    "\n",
    "print(\"Train set accuracy: {}%\".format(accuracy(x_train, y_train, parameters, L, means_per_layer, vars_per_layer) * 100.0))\n",
    "print(\"Test set accuracy: {}%\".format(accuracy(x_test, y_test, parameters, L, means_per_layer, vars_per_layer) * 100.0))\n",
    "\n",
    "# plot the cost\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs (per 5)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b582ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
